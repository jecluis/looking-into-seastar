
NOTES ON SEASTAR
=================

.. note::
    We are trying to follow the tutorial's form, although providing whatever
    additional insights we may have found useful while going through it. A lot
    (if not all?) of the example code is likely grabbed directly (either in
    its original form or slightly adapted) from the tutorial.


---------------
Inner workings
---------------

include/seastar/core/reactor.hpp
    seems to be the main thing, where all the foo is controlled.


From the tutorial [#]_ we get to play, initially, with timers. These examples
focus on getting us to get a broader view of parallelism with futures.

The problem, IMO, with these examples, is that you need to take them at face
value. Yes, you run those examples and you get things happening; we are told
those multiple timers are happening in parallel; it seems that way; but how,
and why?

Digging deeper into the called classes, we go through the `sleep.hh` header;
here we find that the `sleep()` function relies on a `timer` class, which will
work with `std::chrono::steady_timer` (just a monotonic clock, nothing
particularly relevant). The `timer` class, by itself, is nothing amazing, but
there is this one function, called from `sleep()` which is: `timer.arm()`.

Diving into `timer.hh`, we find that this function is actually defined
somewhere else, and that place is in `src/core/reactor.cc`. It seems that it's
in `reactor.cc` that most of the fun is happening.

In `timer<Clock>::arm()`'s definition, we can see that we are arming the timer
(which is basically saying that we started it, etc.), and then we call
`engine().add_timer(this)` -- and this is the interesting bit! At least it seems
that we are getting somewhere that actually tracks these things!

And right enough, `engine()` is a function defined at `core/reactor.hh`,
which returns a `reactor` class.

In its stead, `add_timer()`, defined in `src/core/reactor.cc`, will queue the
timer on a `timer_set` class, which is essentially a list of timers, defined
in `core/timer-set.hh`.

Now we know where these things go, where they are kept. The question that
remains is how we get all this running.

During the tutorial we are told that we shall first create a
`seastar::app_template`, which will start the main event loop (i.e., the
`seastar engine`), on one or more CPUs. We now know this `engine` is the
`reactor` class, but it still begs the question of how we get there.


.. code-block:: cpp
    :linenos:

        int main(int argc, char *argv[]) {
            seastar::app_template app;
            app.run(argc, argv, [] {
                        std::cout << "hello world" << std::endl;
                        return seastar::make_ready_future<>();
                    }
            );
            return 0;
        }


It's that tidbit, on line 3, that is supposed to run the a function (in this
case, a lambda), on the engine. At this point, we are not sure how this is
going to run on multiple cores, but we are told that this is not supposed to
be running in parallel anyway (other examples, later in time, will show
parallel calls). What we do know, is that we are calling `app_template::run()`
and we want to understand what is happening there.

The truly annoying thig is that `class app_template` itself does not initiate
`class reactor`; it simply uses it during `app_template::run()` (via the
`engine()` static function), and that's pretty much it.

Taking a closer look, however, shows a fudgy, somewhat obscure-ish nuance:
the `run()` function is actually returning the result of a call to
`app_template::run_deprecated()` (what's up with that name is beyond me), and
it's in this function that we will run the function passed to `run()`.
Somewhere in this function we are calling `smp::configure()`, which will
indeed *initialize seastar*.

Now, `class smp` is basically a collection of `static` functions and member
variables. Within it we have this one vector of reactors, `static
std::vector<reactor*> _reactors`. This vector will be sized according to the
number of available CPUs, and each position will be populated by and a reactor
class. This is achieved by running a loop (for each CPU), allocating an engine
(via `smp::allocate_reactor()`), and populating a position in the vector. Each
reactor will be configured (`reactor::configure()`), and run
(`reactor::run()`).

At last, `app_template::run_deprecated()` will run the provided function. This
bit leaves us confused, as we are not entirely sure what kind of c++ foo is
happening that allows the function to run, but somehow the `func.get()` in one
of the `then(...)` continuations for the engine will get the function in the
engine's task queue, which will (likelly) be handled during the call to
`engine().run()`.


-----------------------------
On Futures and Continuations
-----------------------------

As we understand, the concept of a `future` is something that will run and
return a value, we just have no idea when that value will be available.
Nonetheless, our execution will continue without blocking waiting for said
value; that is, until we actually *need* that value, at which time we will
request that value, and wait for it.

A very simple example that comes into mind is how we write things on disk on
the monitors. On `Paxos::begin()` one can see that the leader will first
commit the paxos transaction to disk, and only then send it to the peons,
asking them to commit the transaction to disk. We could, instead, rely on a
future: the disk write would be encapsulated in a future, which would return
immediately, and we could move on to sending the `OP_BEGIN` to the peons. This
would not break the algorithm because we would wait for the future's
completion before sending `COMMIT` messages (as well as ensuring we've got all
the `ACCEPT` from a majority of quorum. This may not be a huge performance
improvement, but we would not have to wait for disk I/O which, for larger
writes (e.g., osdmaps), may take longer than the network communication
(although the peons will have to wait for their writes before replying with
the `ACCEPT` message, but we may still end up writing the leader's data in
parallel with the peons; and release the leader to keep doing other things in
the mean time).

Now, that is a future. A continuation is something that is chained to the
completion of a future. E.g.,


.. code-block:: cpp
    :linenos:

        #include <seastar/core/sleep.hh>
        #include <iostream>

        seastar::future<int> slow() {
            using namespace std::chrono_literals;
            return seastar::sleep(100ms).then([] { return 3; });
        }

        seastar::future<> f() {
            return slow().then([] (int val) {
                std::cout << "got " << val << std::endl;
            });
        }


Taking a look at `f()`, we see that we are calling `slow()`, which will
explicitely return an integer future. Given a future is being returned, we can
simply move on, because we know that whatever is being run will be available
at a later time. This call, however, is chained to a continuation that will
print `got <int>`. Once the `slow()` future returns, we run this continuation,
which will receive an integer as an argument and print `got 3`.

Given how we have chained these futures, we can also write function `f()` as


.. code-block:: cpp
    :linenos:

        seastar::future<> f() {
            using namespace std::chrono_literals;
            return seastar::sleep(100ms).then([] {
                return 3;
            }).then([] (int val) {
                std::cout << "got " << val << std::endl;
            });
        }


Following the tutorial, we are now presented with a `ready future`. This seems
as an optimization, to instruct make its return immediately available. We are
told that most continuations are executed sequentially, without having to wait
in the event loop, although we assume that this is for those that are simple
and fast enough to execute, that by the time the next continuation is handled
the value of the previous future is already available. However, we are also
told that there is a limit on those, so that the event loop (the `engine
run()` function we talked about before) is not starved; hence it may be
desirable to call on `seastar::make_ready_future()` to make it clear that our
value is ready to be consumed by the next continuation.


-------------------
Exception Handling
-------------------

The framework will mimic its exception handling to what would be expected from
a sequential execution thread, even though it may happen within chained
futures/continuations. E.g., for an exception thrown in the code below, the
call to `line4()` will be skipped, much like if we were executing `line4()`
after `line2()` that may have thrown an unhandled exception.

.. code-block:: cpp
    :linenos:

    return line1().then([] {
        return line2(); // throws
    }).then([] {
        return line4(); // skipped
    });


However, for this to work so seastar is able to handle the exception, it
requires the exceptions to be thrown using an `exception future`; otherwise,
it won't be able to automatically deal with `std::exception`. For instance,
futures can have, aside from `then()` continuations, a `finally()`
continuation; the latter is executed whenever the chained futures finish,
regardless of whether an exception has been thrown or not (usually good to
clean up resources). However, we will only reach it if we are handling a
seastar-recognizable exception; otherwise, `finally()` is not called. E.g.,


.. code-block:: cpp
    :linenos:

    seastar::future<> fail_exception_future() {
        return seastar::make_exception_future<>(whatever_exception());
    }

    seastar::future<> fail_boom() {
        throw std::exception("boom");
        return seastar::make_ready_future<>();
    }

    seastar::future<> fail_wrapped_boom() {
        try {
            fail_boom();
        } catch (...) {
            return seastar::make_exception_future(std::current_exception());
        }
        return seastar::make_ready_future<>();
    }

    seastar::future<> f_1() {
        return fail_exception_future().finally([] {
            std::cout << "clean up" << std::endl;
        });
    }

    seastar::future<> f_2() {
        return fail_boom().finally([] {
            std::cout << "this is not executed" << std::endl;
        });
    }

    seastar::future<> f_3() {
        return fail_wrapped_boom().finally([] {
            std::cout << "boomed, but we got here" << std::endl;
        });
    }


In these examples, only `f_2()` will kill the engine before running its
`finally()` continuation. The engine will always fail, mind you, but that is
because the future exception is not being handled.


.. [#] https://github.com/scylladb/seastar/blob/master/doc/tutorial.md
